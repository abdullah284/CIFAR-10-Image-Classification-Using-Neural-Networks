{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"EhWxN47YkOjx"},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","from torch import nn\n","from torch.nn import functional as F\n","\n","import my_utils as mu\n","from IPython import display\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","#Importing the packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w3ZE6ojKs9Wp"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","import sys\n","sys.path.append('/content/gdrive/MyDrive/Colab Notebooks')"]},{"cell_type":"markdown","metadata":{"id":"el04UgQT1xSH"},"source":["READING THE DATASET AND REATING DATALOADERS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4_X3sTdJkZ9S"},"outputs":[],"source":["transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])     \n","\n","batch_size = 50 # number of samples for each batch that will be used in the training\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,    #This loads the training set\n","                                          shuffle=False, num_workers=2)  \n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,        #This loads the validation set\n","                                         shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck') #These are the 10 classes for cifar 10"]},{"cell_type":"markdown","metadata":{"id":"FVdfDT2f1CVH"},"source":["*MODEL*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_9HIOLBZgbh_"},"outputs":[],"source":["class Block(nn.Module):\n","    def __init__(self, num_convs, input_channels, output_channels):\n","        super(Block, self).__init__()\n","        self.num_convs = num_convs  #it's better to define the values in self.num_convs rather than just num_convs, as it makes it easier for the model to obtain for later use\n","        for i in range(num_convs):    \n","            self.add_module('conv{0}'.format(i), nn.Conv2d(input_channels,     #a convolution function is defined for num_convs\n","                                                           output_channels, kernel_size=3, padding=1))\n","            input_channels = output_channels       #we need to ensure that the number of input to the next layer is equivalent to the number of outputs\n","                                                      # from the previous\n","            self.add_module('relu{0}'.format(i), nn.ReLU())\n","                #these layers are added to a dictionary within the class, so they can be exracted in the for loop...\n","                #... of the forward function\n","\n","\n","            # The relu function converts all negative values to zero. Important for transition of gradient with respect...\n","            # ...to the model parameters\n","            \n","        self.avg_pool = nn.AvgPool2d(kernel_size=2,stride=2)    #pooling is used to reduce the number of spatial dimensions, ulitmately giving a larger receptive field...\n","                                                                  #...which allows us to have better focus on more abstract features\n","\n","\n","        \n","    def forward(self, x):   \n","        out = x\n","        for i in range(self.num_convs):    #iterats through the number of convolutions which we have given through the parameters\n","            out = self._modules['conv{0}'.format(i)](out)    \n","            out = self._modules['relu{0}'.format(i)](out)\n","            \n","        \n","        out = self.avg_pool(out)\n","\n","\n","\n","        return out  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"djbGUtAlgcOv"},"outputs":[],"source":["class Block_Manager(nn.Module):\n","    def __init__(self, conv):\n","        super(Block_Manager, self).__init__()\n","        in_channels = 3     # due to the dataset containing rgb images, we have 3 channels\n","        self.conv_arch = conv\n","        for i, (num_convs, out_channels) in enumerate(conv):\n","\n","# this iterates through the convs parameters we defined. each convs is a block for the model. \n","# the parameters we use are for the model are 2 blocks, each with 1 convolutional layer\n","\n","          \n","            self.add_module('Block{0}'.format(i), Block(num_convs, in_channels, out_channels))\n","            in_channels = out_channels\n","\n","        self.last = nn.Sequential(nn.Flatten(), nn.Linear(192, 1024),   #this is the mlp layer, at the end of the backbone, which prepares the model to go through...\n","                                                                           # ...the softmax regressiom classifier which is defined in the loss function  \n","                                  nn.ReLU(), nn.Linear(1024,1024), \n","                                  nn.ReLU(),  nn.Linear(1024, 10))    #the final layer return 10 values which go into the softax\n","                                                          #...classifer int he loss function. here the image classification for each each sample is predicted\n","                                                                      \n","        \n","        #Sequencing makes it neater to define a list of layers\n","\n","\n","        \n","    def forward(self, x):\n","        out = x\n","        for i in range(len(self.conv_arch)):\n","            out = self._modules['Block{0}'.format(i)](out)\n","        out = self.last(out)\n","        return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j-7u45ahVS-H"},"outputs":[],"source":["convs = ((1, 3) , (1,3))      #this model will have 2 blocks in the backbone, each with 1 convolutional layer\n","net = Block_Manager(convs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jx61ZomKrm9w"},"outputs":[],"source":["def init_weights(m):\n","    if isinstance(m, nn.Linear): \n","        torch.nn.init.normal_(m.weight, std=0.01)\n","        torch.nn.init.zeros_(m.bias)\n","\n","net.apply(init_weights);\n","print(net)"]},{"cell_type":"markdown","metadata":{"id":"SAn6VnM2rsl6"},"source":["CREATING LOSS AND OPTIMIZER\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yEhYTQAQlbX2"},"outputs":[],"source":["import torch.optim as optim\n","\n","loss = nn.CrossEntropyLoss()\n","#This essentially calculates the distance between the predicted and actual values of Y. It's better to use this instead...\n","# ...of MSE loss as it optimizes the maximum liklihood in predicting correct values. It takes into account confidence when prediciting the correct value\n","\n","wd = 0.0005\n","lr=0.002\n","\n","\n","\n","optimizer = optim.Adam(net.parameters(), weight_decay = wd , lr=lr)\n","\n","#Weight deay reduces the weight values that are very large which could dominate the outcome. So it's used to make the weights...\n","#...more uniform and prevent overfitting\n","\n","\n","\n","#this is used to updates the weights of the mmodel\n"]},{"cell_type":"markdown","metadata":{"id":"9cZRn6n6qvNx"},"source":["EVALUATION FUNCTIONS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1hbrxBeuqpHN"},"outputs":[],"source":["def accuracy(y_hat, y):  \n","    \"\"\"Compute the number of correct predictions.\"\"\"\n","    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n","        y_hat = y_hat.argmax(axis=1)     #just in case the values for y is not in the form of labels, and in the form of probabilities...\n","                                         # ...this argmax function converts those probabilities to labels, because we need both the ground truth...\n","                                         #... values of y and the predicted values of y (y_hat) to be labels\n","    cmp = (y_hat.type(y.dtype) == y)  #cmp is a true or false value\n","    return float(torch.sum(cmp))    #the cmp parameter is expressed as 1 if true, and 0 if false.\n","\n","    #This function evaluates the accuracy of 1 sample of data from the minibatch\n","    # we need to add another function below, evaluate_accuracy, which will do this for all samples in the minibatch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YJ6QcikZqpKB"},"outputs":[],"source":["class Accumulator:  \n","    def __init__(self, n):     # \"n\" is the number of metric we want to keep track of\n","        self.data = [0.0] * n    # we will have a list with \"n\" number of zeroes\n","    def add(self, *args):\n","        self.data = [a + float(b) for a, b in zip(self.data, args)]\n","    def reset(self):\n","        self.data = [0.0] * len(self.data)\n","    def __getitem__(self, idx):\n","        return self.data[idx]\n","\n","      #this funciton is called by the evaluate_accuracy function below, it keeps a track of the total number of predictions in each\n","      # minibatch. and the total  number of correct predictions in each minibatch.\n","      # it is essentially tracking the \"accumulated\" sum of total number of predictions and number of correct predictions.\n","      #it is important because we need to take into account the accuracy of all the minibatches, instead of just one minibatch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zKFp3NgeqeH6"},"outputs":[],"source":["def evaluate_accuracy(net, data_iter): \n","    metric = Accumulator(2) \n","    for _, (X, y) in enumerate(data_iter):\n","        metric.add(accuracy(net(X), y), y.numel()) #the metric function keeps track of the accuracy of all the sample data in the minibatch.\n","    return metric[0] / metric[1]\n","\n","  #This function evaluates the accuracy of all the samples of data in the minibatch\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KhTg-4WXqucx"},"outputs":[],"source":["evaluate_accuracy(net, testloader)"]},{"cell_type":"markdown","metadata":{"id":"1CwSUfzqq3jU"},"source":["TRAINING SECTION\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZfJ21r0Tq2Qc"},"outputs":[],"source":["def train_epoch_ch3(net, train_iter, loss, optimizer):  \n","    if isinstance(net, torch.nn.Module):\n","        net.train()\n","    metric = Accumulator(3)\n","    for X, y in train_iter:   #passing the data into the model\n","        y_hat = net(X)\n","        l = loss(y_hat, y)       #use the loss function to find the difference between your predicted value (i.e y_hat) and the real value (y)\n","        optimizer.zero_grad()\n","        l.backward() #calculates the derivative of the loss with respect to the model parameters (i.e gradient of the loss)\n","        optimizer.step()   #this updates the weights (using the formula w -> w - lr*gradient)\n","        metric.add(float(l) * len(y), accuracy(y_hat, y), y.size().numel())   #keeps track of total number of predictions and total number of...\n","        #...correct predictions. and also keeps track of the loss\n","\n","    return metric[0] / metric[2], metric[1] / metric[2] \n","      #metric[0] / metric[2] returns the average loss. metric[1] / metric[2] returns average training accuracy\n","\n","\n","\n","    #this function trains the data for only 1 epoch, i.e 1 pass of the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dwOzWgVZqeJ0"},"outputs":[],"source":["class Animator:  \n","    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n","                 ylim=None, xscale='linear', yscale='linear',\n","                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n","                 figsize=(3.5, 2.5)):\n","        if legend is None:\n","            legend = []\n","        mu.use_svg_display()\n","        self.fig, self.axes = mu.plt.subplots(nrows, ncols, figsize=figsize)\n","        if nrows * ncols == 1:\n","            self.axes = [self.axes, ]\n","        self.config_axes = lambda: mu.set_axes(\n","            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n","        self.X, self.Y, self.fmts = None, None, fmts\n","\n","    def add(self, x, y):\n","        if not hasattr(y, \"__len__\"):\n","            y = [y]\n","        n = len(y)\n","        if not hasattr(x, \"__len__\"):\n","            x = [x] * n\n","        if not self.X:\n","            self.X = [[] for _ in range(n)]\n","        if not self.Y:\n","            self.Y = [[] for _ in range(n)]\n","        for i, (a, b) in enumerate(zip(x, y)):\n","            if a is not None and b is not None:\n","                self.X[i].append(a)\n","                self.Y[i].append(b)\n","        self.axes[0].cla()\n","        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n","            self.axes[0].plot(x, y, fmt)\n","        self.config_axes()\n","        display.display(self.fig)\n","        display.clear_output(wait=True)\n","\n","\n","    # This function plots the average loss, training accuracy and the testing accuracy after each epoch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wS4Cb3NaqeMG"},"outputs":[],"source":["def train_ch3(net, train_iter, test_iter, loss, num_epochs, optimizer):  \n","    animator = Animator(xlabel='epoch', xlim=[0, num_epochs], ylim=[0, 2.5],\n","                        legend=['train loss', 'train acc', 'test acc'])\n","    for epoch in range(num_epochs): #Number of passes through dataset\n","        train_metrics = train_epoch_ch3(net, train_iter, loss, optimizer) # returns the average loss and trainng accuracy, per epoch\n","        test_acc = evaluate_accuracy(net, test_iter) #retuens testing accuracy \n","        animator.add(epoch + 1, train_metrics + (test_acc,))\n","    train_loss, train_acc = train_metrics\n","\n","\n","\n","  \n","# This function trains the model for multiple epochs"]},{"cell_type":"markdown","metadata":{"id":"4LzWJ751wrKE"},"source":["GRAPH REPORT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PfJiwpCLrMIY"},"outputs":[],"source":["num_epochs = 6\n","train_ch3(net, trainloader, testloader, loss, num_epochs, optimizer)\n","# Accuracy: 55%"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNUKftG0EOryYV5Gn+auyMt","gpuType":"T4","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
